import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class VAE(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder1conv1 = nn.Conv2d(3, 64, 3, stride=2, padding=1) #32x32->16x16
        self.encoder1bn1 = nn.BatchNorm2d(64)
        self.encoder1conv2 = nn.Conv2d(64, 128, 3, stride=2, padding=1) #16x16->8x8
        self.encoder1bn2 = nn.BatchNorm2d(128)
        self.encoder1conv3 = nn.Conv2d(128, 256, 3, stride=2, padding=1) #8x8->4x4
        self.encoder1bn3 = nn.BatchNorm2d(256)
        self.encoder1_mu = nn.Linear(256*4*4, 256)
        self.encoder1_logvar = nn.Linear(256*4*4, 256)

        self.encoder2layer1 = nn.Linear(256, 128)
        self.encoder2bn1 = nn.BatchNorm1d(128)
        self.encoder2layer2 = nn.Linear(128, 64)
        self.encoder2bn2 = nn.BatchNorm1d(64)
        self.encoder2_mu = nn.Linear(64, 64)
        self.encoder2_logvar = nn.Linear(64, 64)

        self.decoder2layer1 = nn.Linear(64, 128)
        self.decoder2bn1 = nn.BatchNorm1d(128)
        self.decoder2layer2 = nn.Linear(128, 256)
        self.decoder2bn2 = nn.BatchNorm1d(256)
        self.decoder2_mu = nn.Linear(256, 256)
        self.decoder2_logvar = nn.Linear(256, 256)

        self.decoder1_fc = nn.Linear(256, 256*4*4)
        self.decoder1conv1 = nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1)
        self.decoder1bn1 = nn.BatchNorm2d(128)
        self.decoder1conv2 = nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1)
        self.decoder1bn2 = nn.BatchNorm2d(64)
        self.decoder1conv3 = nn.ConvTranspose2d(64, 3, 3, stride=2, padding=1, output_padding=1)
        self.relu = nn.ReLU()
        self.tanh = nn.Tanh()
    
    def encode1(self, x):
        x = self.relu(self.encoder1bn1(self.encoder1conv1(x)))
        x = self.relu(self.encoder1bn2(self.encoder1conv2(x)))
        x = self.relu(self.encoder1bn3(self.encoder1conv3(x)))
        x = x.view(-1, 256*4*4)
        mu1 = self.encoder1_mu(x)
        logvar1 = torch.clamp(self.encoder1_logvar(x), min=-10, max=10)
        return mu1, logvar1
    
    def encode2(self, mu1, logvar1):
        epsilon1 = torch.randn_like(logvar1)
        std1 = torch.exp(0.5*logvar1)
        z1 = mu1 + std1 * epsilon1
        x = self.relu(self.encoder2bn1(self.encoder2layer1(z1)))
        x = self.relu(self.encoder2bn2(self.encoder2layer2(x)))
        mu2 = self.encoder2_mu(x)
        logvar2 = torch.clamp(self.encoder2_logvar(x), min=-10, max=10)
        return z1, mu2, logvar2

    def decode2(self, mu2, logvar2):
        epsilon2 = torch.randn_like(logvar2)
        std2 = torch.exp(0.5*logvar2)
        z2 = mu2 + std2 * epsilon2
        x = self.relu(self.decoder2bn1(self.decoder2layer1(z2)))
        x = self.relu(self.decoder2bn2(self.decoder2layer2(x)))
        dmu1 = self.decoder2_mu(x)
        dlogvar1 = torch.clamp(self.decoder2_logvar(x), min=-10, max=10)
        return dmu1, dlogvar1
    
    def decode1(self, z1):
        x = self.relu(self.decoder1_fc(z1))
        x = x.view(-1, 256, 4, 4)
        x = self.relu(self.decoder1bn1(self.decoder1conv1(x)))
        x = self.relu(self.decoder1bn2(self.decoder1conv2(x)))
        x = self.tanh(self.decoder1conv3(x))
        return x
    
    def forward(self, x):
        mu1, logvar1 = self.encode1(x)
        z1, mu2, logvar2 = self.encode2(mu1, logvar1)
        dmu1, dlogvar1 = self.decode2(mu2, logvar2)
        pred = self.decode1(z1)
        return pred, z1, mu1, logvar1, mu2, logvar2, dmu1, dlogvar1
    
    def generate(self, num_samples, device):
        z2 = torch.randn(num_samples, 64).to(device)
        x = self.relu(self.decoder2bn1(self.decoder2layer1(z2)))
        x = self.relu(self.decoder2bn2(self.decoder2layer2(x)))
        dmu1 = self.decoder2_mu(x)
        dlogvar1 = torch.clamp(self.decoder2_logvar(x), min=-10, max=10)
        std1 = torch.exp(0.5 * dlogvar1)
        z1 = dmu1 + std1 * torch.randn_like(dmu1)
        return self.decode1(z1)

    @staticmethod
    def alpha_schedule(epoch):
        if epoch < 40:
            alpha = epoch / 40.0
        elif epoch < 50:
            alpha = 1.0
        else:
            alpha = max(0.25, 1.0 - (epoch - 50) / 50.0)
        return alpha


    @staticmethod
    def beta_schedule(epoch):
        if epoch < 20:
            beta = epoch / 20.0
        elif epoch < 30:
            beta = 1.0
        else:
            beta = max(0.4, 1.0 - (epoch - 20) / 100)
        return beta

    @staticmethod
    def gaussian_log_density(z, mu, logvar):
        logvar = torch.clamp(logvar, min=-10, max=10)
        return -0.5 * (math.log(2 * math.pi) + logvar + (z - mu).pow(2) / logvar.exp())

    @staticmethod
    def compute_loss(pred, target, z1, mu1, logvar1, mu2, logvar2, dmu1, dlogvar1):
        recon_loss = F.mse_loss(pred, target, reduction='sum')
        log_p_z1 = torch.sum(VAE.gaussian_log_density(z1, dmu1, dlogvar1))
        log_q_z1 = torch.sum(VAE.gaussian_log_density(z1, mu1, logvar1))
        middle_term = log_p_z1 - log_q_z1

        kl_z2 = -0.5 * torch.sum(1 + logvar2 - torch.pow(mu2, 2) - torch.exp(logvar2))
        return recon_loss, middle_term, kl_z2
    
